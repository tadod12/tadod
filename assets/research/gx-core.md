# Introduction to GX Core

## GX Core overview

### GX environment

A **Data Context** manages the settings and metadata for a GX workflow. The Data Context provides access to the configurations, metadata, and actions of your GX workflow components and the results of data validations

### Connect to data

A **Data Source** is the GX representation of a data store, tells GX how to connect to your data, and supports connection to different types of data stores, include databases, schemas, data files, ...

A **Data Asset** is a collection of records within a Data Source

A **Batch Definition** tells GX how to organize the records within a Data Asset. The Batch Definition Python object enables you to retrieve a **Batch**, or collection of records from a Data Asset, for validation at runtime. A Data Asset can be validated as a single Batch, or partitioned into multiple Batches for separate validations

### Define Expectations

An **Expectation** is a verifiable assertion about data

An **Expectation Suite** is a collection of Expectations. Expectation Suite can be used to validate a Batch of data using multiple Expectations, streamlining the validation process. You can define multiple Expectation Suites for the same data to cover different use cases, and you can apply the same Expectation Suite to different Batches

### Run Validations

A **Validation Definition** explicitly associates a Batch Definition to an Expectation Suite, defining what data should be validated against with Expectations

A **Validation Result** is returned by GX after data validation

A **Checkpoint** is the primary means for validating data in a production deployment of GX. Checkpoints enable you to run a list of Validation Definitions with shared parameters. Checkpoints can be configured to run Actions, and can pass Validation Results to a list of predefined Actions for processing.

**Actions** provide a mechanism of integrate Checkpoints into your data pipeline infrastructure by automatically processing Validation Results. Typical use cases:
- Sending email alerts, Slack/MS Teams messages
- Custom notifications based on the result of data validation

**Data Docs** are human-readable documentation generated by GX that host your Expectation Suite definitions and Validation Results

## Try GX Core

[Validate data in a DataFrame](test/valid_df_pandas.py)

[Validate data in a SQL table](test/valid_sql_table.py)

# Set up a GX environment

## Install Python

GX Requires Python, version 3.9 to 3.12

    python --version

    python -m venv env

## Install GX

    .\env\Scripts\activate

    python -m ensurepip --upgrade

    pip install great_expectations

    import great_expectations as gx

    print(gx.__version__)

## Install additional dependencies

[Install](https://docs.greatexpectations.io/docs/core/set_up_a_gx_environment/install_additional_dependencies)

- Amazon S3
- Nicrosoft Azure Blob Storage
- Google Cloud Storage
- SQL databases
- Spark

        python -m pip install 'great_expectations[spark]'

## Create a Data Context

A Data Context defines the storage location for metadata, such as your configurations for Data Sources, Expectation Suites, Checkpoints, Data Docs, ...

Available Data Context types

- `context = gx.get_context(mode="file")` - File Data Context - A persistent Data Context that stores metadata and configuration information as YAML files within a file system. File Data Contexts allow you to re-use previously configured Expectation Suites, Data Sources, and Checkpoints

- `context = gx.get_context(mode="ephemeral")` - Ephemeral Data Context - A temporary Data Context that stores metadata and configuration information in memory

- `context = gx.get_context(mode="cloud")` - GX Cloud Data Context - A Data Context that connects to a GX Cloud Account to retrieve and store GX Cloud metadata and configuration information. The GX Cloud Data Context lets you leverage GX Cloud to share your Expectation Suites, Data Sources, and Checkpoints with your organization

Create a Cloud Data Context

    import great_expectations as px

    context = gx.get_context(mode="file", project_root_dir="./new_context_folder")
    context = gx.get_context(mode="ephemeral")
    context = gx.get_context(mode="cloud")

# Connect to data

## Connect to SQL data

To connect to your SQL data
- Create a Data Source - tells GX where your database resides and how to connect to it
- Configure Data Assets for your Data Source - tells GX which sets of records you want to be able to access from your Data Source
- Define Batch Definitions - allows you to request all the records retrieved from a Data Asset or further partition the returned records based on the contents of a date and time field

## Connect to Filesystem data

[Link](https://docs.greatexpectations.io/docs/core/connect_to_data/filesystem_data/?environment=filesystems)

## Connect to data in Dataframes

[Connect to Spark](test/spark.py)

# Define Expectations

## Create an Expectation

An Expectation is a verifiable assertion about your data

    import great_expectations as gx

    context = gx.get_context()
    set_up_context_for_example(context)

    # All Expectations are found in the `gx.expectations` module.
    # This Expectation has all values set in advance:
    preset_expectation = gx.expectations.ExpectColumnMaxToBeBetween(
        column="passenger_count", min_value=1, max_value=6
    )

    # In this case, two Expectations are created that will be passed
    #  parameters at runtime, and unique lookups are defined for each
    #  Expectations' parameters.

    passenger_expectation = gx.expectations.ExpectColumnMaxToBeBetween(
        column="passenger_count",
        min_value={"$PARAMETER": "expect_passenger_max_to_be_above"},
        max_value={"$PARAMETER": "expect_passenger_max_to_be_below"},
    )
    fare_expectation = gx.expectations.ExpectColumnMaxToBeBetween(
        column="fare",
        min_value={"$PARAMETER": "expect_fare_max_to_be_above"},
        max_value={"$PARAMETER": "expect_fare_max_to_be_below"},
    )

    # A dictionary containing the parameters for both of the above
    #   Expectations would look like:
    runtime_expectation_parameters = {
        "expect_passenger_max_to_be_above": 4,
        "expect_passenger_max_to_be_below": 6,
        "expect_fare_max_to_be_above": 10.00,
        "expect_fare_max_to_be_below": 500.00,
    }

## Retrieve a Batch of sample data

GX provides two methods of retrieving sample data for testing or data exploration
- Request a Batch of data from any Batch Definition you have previously configured
- Use the built in `pandas_default` Data Source to read in a Batch of data from a datafile such as a `.csv` or `.parquet` file without first defining a corresponding Data Source, Data Asset, and Batch Definition

1. Batch Definition

Batch Definitions both organize a Data Asset's records into Batches and provide a method for retrieving those records

    # Retrieve Batch Definition
    data_source_name = "my_data_source"
    data_asset_name = "my_data_asset"
    batch_definition_name = "my_batch_definition"
    batch_definition = (
        context.data_sources.get(data_source_name)
        .get_asset(data_asset_name)
        .get_batch_definition(batch_definition_name)
    )

    # If you're using File Data Assets, pass values as strings
    yearly_batch_parameters = {"year": "2019"}
    monthly_batch_parameters = {"year": "2019", "month": "01"}
    daily_batch_parameters = {"year": "2019", "month": "01", "day": "01"}

    # Otherwise, pass values as integers
    integer_daily_batch_parameters = {"year": 2019, "month": 1, "day": 1}

    batch = batch_definition.get_batch(batch_parameters={"year": "2019", "month": "01"})

2. pandas_default

The pandas_default Data Source is built into every Data Context and can be found at `.data_sources.pandas_default` on your Data Context

    import great_expectations as gx

    context = gx.get_context()
    set_up_context_for_example(context)

    # Provide the path to a data file:
    file_path = "./data/folder_with_data/yellow_tripdata_sample_2019-01.csv"

    # Use the `pandas_default` Data Source to read the file:
    sample_batch = context.data_sources.pandas_default.read_csv(file_path)

    # Verify that data was read into `sample_batch`:
    print(sample_batch.head())

## Test an Expectation

    import great_expectations as gx

    context = gx.get_context()
    set_up_context_for_example(context)


    # Use the `pandas_default` Data Source to retrieve a Batch of sample Data from a data file:
    file_path = "./data/folder_with_data/yellow_tripdata_sample_2019-01.csv"
    batch = context.data_sources.pandas_default.read_csv(file_path)

    # Define the Expectation to test:
    expectation = gx.expectations.ExpectColumnMaxToBeBetween(
        column="passenger_count", min_value=1, max_value=6
    )

    # Test the Expectation:
    validation_results = batch.validate(expectation)

    # Evaluate the Validation Results:
    print(validation_results)

    # If needed, adjust the Expectation's preset parameters and test again:
    expectation.min_value = 1
    expectation.max_value = 6

    # Test the modified expectation and review the new Validation Results:
    new_validation_results = batch.validate(expectation)
    print(new_validation_results)

## Organize Expectations into an Expectation Suite

[Code](gx.ipynb)

# Run Validations

## Create a Validation Definition

A Validation Definition is a fixed reference that links a Batch of data to an Expectation Suite

[Validation Definition](test/valid_def.py)

## Run a Validation Definition

    # Define Batch parameters
    # Accepted keys are determined by the BatchDefinition used to instantiate this ValidationDefinition.
    batch_parameters_dataframe = {"dataframe": df}
    batch_parameters_daily = {"year": "2020", "month": "1", "day": "17"}
    batch_parameters_yearly = {"year": "2019"}

    # Run the Validation Definition
    validation_results = validation_definition.run(batch_parameters=batch_parameters_yearly)

    # Review the Validation Results
    print(validation_results)

# Trigger actions based on results

## Create a Checkpoint with Actions

A Checkpoint executes one or more Validation Definitions and then performs a set of Actions based on the Validation Results each Validation Definition returns.

[Checkpoint with Actions](test/checkpoint.py)

## Create a custom Action

## Choose result format

When you validate data with GX Core you can set the level of detail returned in your Validation Results by specifying a value for the optional `result_format` parameter

### Define a Result Format configuration

The `result_format` parameter takes in a dictionary of configuration settings

#### BOOLEAN_ONLY

    boolean_result_format_dict = {"result_format": "BOOLEAN_ONLY"}

The successful evaluation of the Expectation is exclusively returned via the `True` or `False` value of the `success` key in the returned Validation Result


